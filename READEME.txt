*******************************************
Assinment 2 -
Collocation Extraction

*******************************************

Tom Horvitz 203062567
Amiram Lifshitz 304992381

*******************************************

Instruction -
	1. create bucket name - "tom-amiram"
	2. upload firstjob.jar, secondjob.jar, thirdjob.jar
	3. run in the command line 
		java -jar main.jar [min PMI] [Relative NPMI]
	4. the code will open new folder in "tom-amiram" bucket for all steps, logs and outputs.
	
	
the output will show you the pair of ward greater the min PMI or Relative NPMI
********************************************

AWS -

S3 Bucket-
	"tom-amiram" :main bucket contain: output files, firstjob.jar , secondjob.jar and thirdjob.jar
	
*********************************************

project -

main.java :
	this class is local, you need to run it on your local machine
	args- min PMI, relative min PMI
	at first it create unick uid and open folder on the main bucket "tom-amiram"
	all the files will create on this folder
	the class will get AWS CredentialsProvider
	and will use hadoop to manage the 3 steps and run them- firstjob, secondjob, thirdjob

firstjob.jar :
	mapper:
		read the input file:
			w1 = first object
			w2 = second object
			year = third object
			amount = forth object
		create 4 KV:
			     /*
	             * for each pair we write 4 jobs
	             * 1) <*,*,decade>: counting everything in the decade 
	             * 2) <firstWord,*,decade>: counting first word
	             * 3) <*,secWord,decade>: counting second word
	             * 4) <firstWord,secWord,decade>: counting both words together
	             */
			context.write(new FirstJobKey(ast,ast,decade), new LongWritable(amount));// 1
	        context.write(new FirstJobKey(w1,ast,decade), new LongWritable(amount));// 2
	        context.write(new FirstJobKey(ast,w2,decade), new LongWritable(amount));// 3
	        context.write(new FirstJobKey(w1,w2,decade), new LongWritable(amount)); // 4
	reducer:
		if(key = <*,*,decade>):
			create new file name "decade amount" in folder N
			and upload to S3
		if(key = <w1,*,decade>):
			take the value and save him on global variable for using it next for C(w1)
		if(key = <*,w2,decade>):
			send to next job:
				for calculate the C(w2)
				context.write(new FirstJobKey(w2,*,decade), new LongWritable(0,amount));
		if(key = <w1,w2,decade>):
			the the global variable for C(w1) and send to next job
				context.write(new FirstJobKey(w2,w1,decade), new LongWritable(amount,C(w1)));



secondjob.jar :
	mapper:
		send to reducer as he get it:
        	context.write(key, value);
	reducer:
		setup:
			read all the N's files and put on an hashmap the decade with the amount of NPMI
		if(key = <w2,*,decade>):
			put the amount on global variable for the C(w2)
		if(key = <w1,w2,decade>):
			get all values for the NPMI equasion:
				C(w1), C(w2), C(w1,w2) , NPMI
			send to the third job all those values
			
thirdjob.jar:
		mapper:
			calculate for every pair the value of NPMI
			<w1,w2,decade> +  NPMI send the result to the reducer 
			<*,*,decade> + C(w1,w2) send the C(w1,w2) for calculate the sum of NPMI in the same decade
		reducer:
			if(key = <*,*,decade>):
				sum all the C(w1,w2) to a global variable for the sum of NPMI
			if(w1,w2,decade):
				calculate the relative NPMI
				check if the input NPMI or rel NPMI are lower than the get NPMI and send the result to output.
	
		
